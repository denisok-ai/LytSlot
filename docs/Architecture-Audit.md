# Полный аудит архитектуры LytSlot Pro

Оценка текущей архитектуры с точки зрения отладки и эксплуатации, а также анализ целесообразности ухода от Python.

---

## 1. Текущая архитектура (сводка)

| Компонент | Стек | Назначение |
|-----------|------|------------|
| **API** | FastAPI (Python), sync SQLAlchemy | REST, auth, CRUD, tenant (RLS), админ |
| **Worker** | Celery (Python), Redis broker | Публикация в TG, уведомления, аналитика |
| **Bot** | aiogram 3 (Python), FSM в памяти | Один бот на всех, заказ слотов через Telegram |
| **Web** | Next.js 14 (TypeScript) | Дашборд, календарь, аналитика |
| **БД** | PostgreSQL + TimescaleDB (views) | Данные, RLS по tenant_id |
| **Инфра** | Docker/Podman, docker-compose | Контейнеры, один compose на всё |

**Границы:** API и Worker разделены (очередь). Bot и Web — клиенты API. Общая БД и Redis. Мульти-тенантность через RLS и JWT tenant_id.

---

## 2. Аудит эффективности отладки

### 2.1 Что уже хорошо

- **Чёткое разделение API / Worker / Bot** — можно воспроизводить сценарии по отдельности (API без worker, worker без bot).
- **Один монорепо** — общие модели (db, shared), консистентные контракты.
- **RLS и tenant_id** — изоляция данных предсказуема, проще искать утечки по tenant.
- **Опциональный worker** (после доработок) — API работает без Redis/Celery, проще локальный дебаг.

### 2.2 Слабые места для отладки

| Проблема | Влияние | Связано с языком? |
|----------|---------|-------------------|
| **Нет сквозного request_id / trace_id** | Сложно связать запрос API → задача Celery → лог. | Нет. Решается middleware + передачей ID в задачи. |
| **Логи не структурированы (JSON)** | Трудно парсить и фильтровать в Loki/ELK. | Нет. Решается structlog/логгером в JSON. |
| **Нет тестов** | Регрессии и отладка «вручную». | Нет. Одна и та же проблема на любом стеке. |
| **Ошибки в worker только в логах** | Нет алертов, нет трейсов в одном месте. | Частично: в Python/Celery нет «из коробки» трейсинга; то же в Node/Go без инструментации. |
| **Bot FSM в MemoryStorage** | При рестарте бота состояние теряется, сложно воспроизвести сценарий. | Специфика aiogram; смена языка не убирает проблему хранения состояния. |
| **Синхронный API при блокирующем I/O** | Под нагрузкой отладка «подвисаний» сложнее. | Да, Python GIL и sync — но для 10k req/day не критично. |

**Вывод по отладке:** основные пробелы — **инструментация** (trace_id, структурированные логи, тесты, алерты), а не выбор Python. Смена языка без этого не сделает отладку «эффективнее».

---

## 3. Аудит эффективности эксплуатации

### 3.1 Что уже хорошо

- **Один docker-compose** — поднять весь стек одной командой.
- **Health endpoint** (`/health`) на API.
- **Конфиг через env** — один .env для всех сервисов.
- **RLS** — изоляция на уровне БД, меньше риска ошибок в коде.

### 3.2 Слабые места для эксплуатации

| Проблема | Влияние | Связано с языком? |
|----------|---------|-------------------|
| **Нет проверки зависимостей при старте** | API может стартовать при недоступной БД/Redis, ошибки — при первом запросе. | Нет. Решается readiness probe (БД + опционально Redis). |
| **Нет метрик (Prometheus)** | Нет графиков RPS, латентности, очереди Celery. | Нет. Добавляется экспортом метрик в любом стеке. |
| **Нет централизованных ошибок (Sentry и т.п.)** | Сбои видны только в логах контейнеров. | Нет. |
| **Один compose без разделения env prod/dev** | Риск перепутать окружения. | Нет. |
| **Bot и Worker без явного health** | Сложнее проверять живучесть в оркестраторе. | Нет. |
| **Нет runbook/playbook** | Действия при падении БД/Redis/очереди не формализованы. | Нет. |

**Вывод по эксплуатации:** не хватает **observability и операционных практик** (health, метрики, алерты, runbook), а не «другого языка».

---

## 4. Оценка: уход от Python

### 4.1 Что пришлось бы переписать

- **API** (FastAPI, роутеры, auth, RLS, интеграции).
- **Worker** (Celery → аналог на целевом стеке).
- **Bot** (aiogram → другой фреймворк для Telegram).
- **Скрипты** (миграции Alembic, seed) или их эквиваленты.

Фронт (Next.js) и БД (PostgreSQL, RLS) можно не трогать.

### 4.2 Сравнение вариантов для вашего домена

| Критерий | Python (текущий) | Node/TypeScript | Go |
|----------|------------------|------------------|-----|
| **Скорость отладки и итерации** | Высокая (REPL, быстрый цикл, богатый стек для TG) | Высокая | Ниже (компиляция, меньше «живого» дебага) |
| **Отладка асинхронности** | Средняя (sync API, async только в боте) | Хорошая (всё async) | Хорошая (горутины) |
| **Эксплуатация (деплой, память, CPU)** | Норма для 10k req/day | Сопоставимо | Лучше при высокой нагрузке и малом числе инстансов |
| **Очереди и воркеры** | Celery — зрело, много примеров | BullMQ и др. — норма | Свои воркеры или RabbitMQ — больше кода |
| **Telegram-бот** | aiogram — один из лучших вариантов | telegraf — хорошо | Нет доминирующего фреймворка |
| **Мульти-тенантность и RLS** | Не зависят от языка | То же | То же |
| **Риск и стоимость перехода** | — | Высокая (переписать 3 сервиса + скрипты) | Высокая + дольше разработка |

### 4.3 Будет ли «эффективнее» при отладке и эксплуатации?

- **Отладка:** при переходе на Node/Go вы **не получаете** автоматически trace_id, структурированные логи, тесты и нормальный дебаг продакшена. Без них отладка останется такой же или станет дольше из-за нового стека. **Эффективнее станет только если** параллельно внедрить практики (логи, трейсинг, тесты).
- **Эксплуатация:** выигрыш по ресурсам и латентности возможен в основном на **Go** при росте нагрузки (десятки тысяч RPS, жёсткие SLA). Для целевых 10k заказов/день текущий стек по ресурсам достаточен. Стабильность и предсказуемость больше зависят от health checks, метрик, алертов и runbook, чем от языка.

**Итог:** уход от Python сам по себе **не даёт** заметного выигрыша в эффективности отладки и эксплуатации для текущего масштаба. Выигрыш появляется при:
- переходе на Go и значительном росте нагрузки, или
- если команда хочет стандартизироваться на одном стеке (например, только Node/TS для всего бэкенда).

---

## 5. Рекомендации (без смены языка)

Чтобы сделать отладку и эксплуатацию заметно эффективнее **в текущей архитектуре**:

### 5.1 Отладка

1. **Сквозной request_id / trace_id**  
   В API: middleware, который генерирует или прокидывает `X-Request-Id`, пишет в логи. В Celery: передавать этот ID в `args/kwargs` задачи и логировать — по одному ID можно искать и запрос, и задачу.
2. **Структурированные логи (JSON)**  
   Например, `structlog` в API и worker: уровень, сообщение, request_id, tenant_id, время. Удобно искать в Loki/CloudWatch/ELK.
3. **Тесты**  
   Минимум: интеграционные тесты на ключевые сценарии (создание заказа, вызов worker, RLS). Это сильно ускоряет отладку регрессий.
4. **Опционально: APM**  
   Sentry/OpenTelemetry — трейсы запросов и задач, связка с логами по trace_id.

### 5.2 Эксплуатация

1. **Readiness probe API**  
   Эндпоинт (например `/ready`), который проверяет: подключение к БД и при необходимости к Redis. Не отдавать 200, пока зависимости недоступны. Использовать в k8s/Docker для health check.
2. **Метрики Prometheus**  
   В API: счётчики запросов по эндпоинту/статусу, гистограммы латентности. В worker: длина очередей, время выполнения задач. Дашборды в Grafana.
3. **Централизованные ошибки**  
   Интеграция с Sentry (или аналог): необработанные исключения из API и worker уходят в один проект с трейсами и контекстом.
4. **Runbook**  
   Короткий документ: что делать при падении БД, Redis, росте очереди Celery, как перезапускать сервисы и куда смотреть в логах/метриках.

### 5.3 Приоритет

| # | Действие | Эффект для отладки/эксплуатации |
|---|----------|----------------------------------|
| 1 | request_id + структурированные логи | Высокий |
| 2 | Readiness probe + health для зависимостей | Высокий |
| 3 | Тесты (интеграционные) на основные сценарии | Высокий |
| 4 | Метрики Prometheus + дашборд | Средний |
| 5 | Sentry (или аналог) | Средний |
| 6 | Runbook | Средний |

---

## 6. Итоговый вывод

- **Архитектура в целом нормальная:** разделение API / Worker / Bot, RLS, один монорепо. Основные риски — не язык, а отсутствие единого трейсинга, структурированных логов, тестов, метрик и явных операционных процедур.
- **Уход от Python** для текущего масштаба и целей (отладка + эксплуатация) **не даёт явного выигрыша** и несёт высокие затраты на переписывание. Имеет смысл рассматривать только при планах сильного масштабирования (и тогда в первую очередь — Go для API/worker) или при стратегии «один стек» (например, только Node/TS).
- **Максимальный эффект** даёт не смена языка, а внедрение практик: request_id, структурированные логи, readiness, метрики, тесты, Sentry, runbook. Их можно вводить по шагам в текущем Python-стеке.

Если нужно, следующий шаг — детализировать один из пунктов (например, формат логов и пример middleware для request_id) или набросать план внедрения по приоритетам и срокам.
